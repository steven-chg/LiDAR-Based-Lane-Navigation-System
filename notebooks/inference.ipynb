{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan/anaconda3/envs/pointcept/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/yan/anaconda3/envs/pointcept/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from pointcept.models.point_transformer_v3.point_transformer_v3m1_base import PointTransformerV3\n",
    "from pointcept.datasets.transform import (\n",
    "    GridSample, Compose, ToTensor, Collect, RandomRotateTargetAngle, TRANSFORMS\n",
    ")\n",
    "from pointcept.datasets.utils import collate_fn\n",
    "from collections import OrderedDict\n",
    "import pointcept.utils.comm as comm\n",
    "from pointcept.models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(weight_path):\n",
    "    \"\"\"\n",
    "    Load model with specified weights.\n",
    "    \n",
    "    Args:\n",
    "        weight_path: path to model weights\n",
    "    Returns:\n",
    "        model: loaded model in evaluation mode\n",
    "    \"\"\"\n",
    "    # Define model config directly\n",
    "    model_cfg = dict(\n",
    "        type='DefaultSegmentorV2',\n",
    "        num_classes=2,\n",
    "        backbone_out_channels=64,\n",
    "        backbone=dict(\n",
    "            type='PT-v3m1',\n",
    "            in_channels=4,\n",
    "            order=['z', 'z-trans', 'hilbert', 'hilbert-trans'],\n",
    "            stride=(2, 2, 2, 2),\n",
    "            enc_depths=(2, 2, 2, 6, 2),\n",
    "            enc_channels=(32, 64, 128, 256, 512),\n",
    "            enc_num_head=(2, 4, 8, 16, 32),\n",
    "            enc_patch_size=(64, 64, 64, 64, 64),\n",
    "            dec_depths=(2, 2, 2, 2),\n",
    "            dec_channels=(64, 64, 128, 256),\n",
    "            dec_num_head=(4, 4, 8, 16),\n",
    "            dec_patch_size=(64, 64, 64, 64),\n",
    "            mlp_ratio=4,\n",
    "            qkv_bias=True,\n",
    "            qk_scale=None,\n",
    "            attn_drop=0.0,\n",
    "            proj_drop=0.0,\n",
    "            drop_path=0.3,\n",
    "            shuffle_orders=True,\n",
    "            pre_norm=True,\n",
    "            enable_rpe=False,\n",
    "            enable_flash=False,\n",
    "            upcast_attention=False,\n",
    "            upcast_softmax=False,\n",
    "            cls_mode=False,\n",
    "            pdnorm_bn=False,\n",
    "            pdnorm_ln=False,\n",
    "            pdnorm_decouple=True,\n",
    "            pdnorm_adaptive=False,\n",
    "            pdnorm_affine=True,\n",
    "            pdnorm_conditions=('nuScenes', 'SemanticKITTI', 'Waymo'))\n",
    "    )\n",
    "\n",
    "    # Initialize model and load weights\n",
    "    model = build_model(model_cfg)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        checkpoint = torch.load(weight_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(weight_path, map_location='cpu')\n",
    "        \n",
    "    # Load state dict\n",
    "    weight = OrderedDict()\n",
    "    for key, value in checkpoint[\"state_dict\"].items():\n",
    "        if key.startswith(\"module.\"):\n",
    "            key = key[7:]\n",
    "        weight[key] = value\n",
    "    model.load_state_dict(weight, strict=True)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def inference(points, model):\n",
    "    \"\"\"\n",
    "    Run inference on point cloud data.\n",
    "    \n",
    "    Args:\n",
    "        points: numpy array of shape (n,4) - x,y,z,intensity\n",
    "        model: loaded model in evaluation mode\n",
    "    Returns:\n",
    "        pred: predictions\n",
    "        probs: confidence scores\n",
    "    \"\"\"\n",
    "    # Create data dict \n",
    "    coord = points[:, :3]\n",
    "    strength = points[:, -1].reshape([-1, 1])\n",
    "    segment = np.zeros(points.shape[0]).astype(np.int32)\n",
    "    data_dict = dict(coord=coord, strength=strength, segment=segment)\n",
    "\n",
    "    # Define and apply transforms\n",
    "    transform = Compose([\n",
    "        dict(type='Copy', keys_dict=dict(segment='origin_segment')),\n",
    "        dict(\n",
    "            type='GridSample',\n",
    "            grid_size=0.05,\n",
    "            hash_type='fnv',\n",
    "            mode='train',\n",
    "            keys=('coord', 'strength', 'segment'),\n",
    "            return_inverse=True)\n",
    "    ])\n",
    "    data_dict = transform(data_dict)\n",
    "    inverse = data_dict.pop(\"inverse\")\n",
    "\n",
    "    # Apply voxelize transform\n",
    "    voxelize = TRANSFORMS.build(dict(\n",
    "        type='GridSample',\n",
    "        grid_size=0.05,\n",
    "        hash_type='fnv',\n",
    "        mode='test',\n",
    "        return_grid_coord=True,\n",
    "        keys=('coord', 'strength')))\n",
    "    data_part_list = voxelize(data_dict)\n",
    "\n",
    "    # Apply post transform\n",
    "    post_transform = Compose([\n",
    "        dict(type='ToTensor'),\n",
    "        dict(\n",
    "            type='Collect',\n",
    "            keys=('coord', 'grid_coord', 'index'),\n",
    "            feat_keys=('coord', 'strength'))\n",
    "    ])\n",
    "    \n",
    "    fragment_list = []\n",
    "    for data_part in data_part_list:\n",
    "        fragment_list.append(post_transform(data_part))\n",
    "\n",
    "    # Do inference\n",
    "    pred = torch.zeros((points.shape[0], 2))  # num_classes = 2\n",
    "    if torch.cuda.is_available():\n",
    "        pred = pred.cuda()\n",
    "\n",
    "    for fragment in fragment_list:\n",
    "        idx_part = fragment[\"index\"]\n",
    "        \n",
    "        input_dict = {}\n",
    "        for key, value in fragment.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                input_dict[key] = value.cuda() if torch.cuda.is_available() else value\n",
    "            else:\n",
    "                input_dict[key] = value\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_part = model(input_dict)[\"seg_logits\"]\n",
    "            pred_part = F.softmax(pred_part, -1)\n",
    "            \n",
    "            bs = 0\n",
    "            for be in input_dict[\"offset\"]:\n",
    "                pred[idx_part[bs:be], :] += pred_part[bs:be]\n",
    "                bs = be\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Get final predictions and probabilities \n",
    "    probs = pred.max(1)[0].cpu().numpy()\n",
    "    pred = pred.max(1)[1].cpu().numpy()\n",
    "\n",
    "    # Map back to original points\n",
    "    pred = pred[inverse]\n",
    "    probs = probs[inverse]\n",
    "\n",
    "    return pred, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data...\n",
      "Loading model...\n",
      "Processing 568 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 568/568 [02:04<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "Results saved in: predictions\n",
      "Input shape: (568, 128, 1024, 7)\n",
      "Output predictions shape: (568, 131072)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def load_npy_data(path):\n",
    "    \"\"\"\n",
    "    Load .npy file data.\n",
    "    \n",
    "    Args:\n",
    "        path: path to .npy file with shape (N, 128, 1024, 7)\n",
    "    Returns:\n",
    "        data: array of shape (N, 128, 1024, 7)\n",
    "    \"\"\"\n",
    "    return np.load(path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    input_path = \"/home/yan/slam06_mapping_local.npy\"  # Replace with your input path\n",
    "    weight_path = \"/home/yan/pointcept151/exp/nuscenes/train_highbay_02/model/model_best.pth\"\n",
    "    output_dir = \"predictions\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load input data\n",
    "    print(\"Loading input data...\")\n",
    "    data = load_npy_data(input_path)\n",
    "    num_frames = data.shape[0]\n",
    "    \n",
    "    # Load model (only once)\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model(weight_path)\n",
    "    \n",
    "    # Process each frame\n",
    "    print(f\"Processing {num_frames} frames...\")\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    for frame_idx in tqdm(range(num_frames)):\n",
    "        # Get single frame data\n",
    "        frame_data = data[frame_idx]  # Shape: (128, 1024, 7)\n",
    "        \n",
    "        # Reshape for inference\n",
    "        points = frame_data.reshape(-1, frame_data.shape[-1])  # Shape: (128*1024, 7)\n",
    "        # normalizing and adjusting the near_ir values\n",
    "        points[:,6] = (points[:,6]*10)/65532\n",
    "        # Run inference\n",
    "        # start_time = time.time()\n",
    "        pred, probs = inference(points, model)\n",
    "        # end_time = time.time()\n",
    "        # print(f\"Frame {frame_idx} - Inference time: {end_time - start_time:.4f} seconds\")\n",
    "        \n",
    "        # Reshape predictions and probabilities back to (128, 1024)\n",
    "        # pred = pred.reshape(128, 1024)\n",
    "        # probs = probs.reshape(128, 1024)\n",
    "        \n",
    "        # Save individual frame results\n",
    "        # np.save(os.path.join(output_dir, f'pred_frame_{frame_idx:04d}.npy'), pred)\n",
    "        # np.save(os.path.join(output_dir, f'prob_frame_{frame_idx:04d}.npy'), probs)\n",
    "        \n",
    "        # Collect results\n",
    "        all_predictions.append(pred)\n",
    "        all_probabilities.append(probs)\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)  # Shape: (N, 128, 1024)\n",
    "    all_probabilities = np.array(all_probabilities)  # Shape: (N, 128, 1024)\n",
    "    \n",
    "    # Save combined results\n",
    "    np.save(os.path.join(output_dir, 'slam06_predictions.npy'), all_predictions)\n",
    "    # np.save(os.path.join(output_dir, 'all_probabilities.npy'), all_probabilities)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Results saved in: {output_dir}\")\n",
    "    print(f\"Input shape: {data.shape}\")\n",
    "    print(f\"Output predictions shape: {all_predictions.shape}\")\n",
    "    # print(f\"Output probabilities shape: {all_probabilities.shape}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    # print(\"\\nPrediction statistics:\")\n",
    "    # print(f\"Unique classes: {np.unique(all_predictions)}\")\n",
    "    # print(f\"Mean confidence: {np.mean(all_probabilities):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568, 131072)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load('/home/yan/pointcept151/predictions/all_probabilities.npy')\n",
    "data = np.load('/home/yan/pointcept151/predictions/slam06_predictions.npy')\n",
    "data.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([27645901, 55191603]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_predictions,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.random.rand(1000, 4)\n",
    "# Load pointcloud\n",
    "# points = np.fromfile(points_in, dtype=np.float32).reshape(-1, 4)\n",
    "path='/home/yan/pointcept151/dataset/sequences/00/velodyne/000200.bin'\n",
    "#read the point cloud data\n",
    "def read_bin(path):\n",
    "    pc = np.fromfile(path, dtype=np.float32).reshape(-1, 4)\n",
    "    return pc\n",
    "test_points = read_bin(path)\n",
    "weight_path=\"/home/yan/pointcept151/exp/nuscenes/train_highbay_02/model/model_best.pth\"\n",
    "# Run inference\n",
    "pred, probs = inference(test_points, weight_path)\n",
    "np.save('pred.npy',pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointcept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
